{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Qwen2-72B-Instruct-AWQ deployment guide\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b5c86ea-a76e-4c43-96f0-4b97d83f76ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# option_model_dir=s3://sagemaker-us-west-2-452145973879/models/Qwen2-72B-Instruct-AWQ/\n",
    "# s3://sagemaker-us-west-2-452145973879/models/Qwen2-72B-Instruct-AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b011bf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.model_id=s3://sagemaker-us-west-2-452145973879/models/Qwen2-72B-Instruct-AWQ/\n",
    "option.dtype=fp16\n",
    "option.quantize=awq\n",
    "option.task=text-generation\n",
    "option.rolling_batch=vllm\n",
    "option.tensor_parallel_degree=4\n",
    "option.max_model_len=25456\n",
    "option.device_map=auto\n",
    "option.enable_streaming=true\n",
    "# Adjust the following based on model size and instance type\n",
    "option.max_rolling_batch_size=16\n",
    "option.output_formatter=jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0142973",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel/\n",
      "mymodel/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "[Large Model Inference available DLC](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_uri = image_uris.retrieve(\n",
    "#         framework=\"djl-deepspeed\",  # \"djl-deepspeed\"\n",
    "#         region=sess.boto_session.region_name,\n",
    "#         version=\"0.27.0\"\n",
    "#     )\n",
    "\n",
    "image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-452145973879/Qwen2-72B-Instruct-AWQ/code/mymodel.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"Qwen2-72B-Instruct-AWQ/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### 4.2 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e61cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b332fa7-5612-4fc9-aeb5-781e40e1640e",
   "metadata": {},
   "source": [
    "## if the model has already been deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04bd88a6-29ec-4f44-87d1-c2b7297a754e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.base_predictor.Predictor at 0x7f733f5ed240>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sagemaker\n",
    "\n",
    "# endpoint_name = 'lmi-model-2024-06-11-09-18-56-099'\n",
    "# predictor = sagemaker.Predictor(\n",
    "#     endpoint_name=endpoint_name, \n",
    "#     sagemaker_session=sess,\n",
    "#     serializer=serializers.JSONSerializer()\n",
    "# )\n",
    "\n",
    "# predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79786708",
   "metadata": {},
   "source": [
    "Firstly let's try to run with a wrong inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e15d31e-44c8-4c9c-a1f3-bea0a43448e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:  {\"generated_text\": \"{\\\"customerType\\\": \\\"个人\\\", \\\"customerName\\\": \\\"余先生\\\", \\\"phoneNumber1\\\": \\\"90256922\\\", \\\"phoneNumber2\\\": \\\"25687021\\\", \\\"email\\\": \\\"\\\", \\\"address\\\": \\\"柴灣道111號高威港第四座四座八樓私宅\\\", \\\"productBrand\\\": \\\"\\\", \\\"productCategoryName\\\": \\\"抽濕機\\\", \\\"serialNumber\\\": \\\"RADY200H\\\", \\\"srType\\\": \\\"维修\\\", \\\"srSubType\\\": \\\"维修\\\", \\\"symptomDescription\\\": \\\"濕濕咗嘅時候呢嗰啲水呢直接流落去地下, 接不到那些水\\\", \\\"customerRequest\\\": \\\"\\\", \\\"refNo\\\": \\\"\\\", \\\"selloutInvoiceNum\\\": \\\"\\\", \\\"salesDealerName\\\": \\\"\\\", \\\"installDealerName\\\": \\\"\\\"}\"}\n",
      "generated_text:  {\"customerType\": \"个人\", \"customerName\": \"余先生\", \"phoneNumber1\": \"90256922\", \"phoneNumber2\": \"25687021\", \"email\": \"\", \"address\": \"柴灣道111號高威港第四座四座八樓私宅\", \"productBrand\": \"\", \"productCategoryName\": \"抽濕機\", \"serialNumber\": \"RADY200H\", \"srType\": \"维修\", \"srSubType\": \"维修\", \"symptomDescription\": \"濕濕咗嘅時候呢嗰啲水呢直接流落去地下, 接不到那些水\", \"customerRequest\": \"\", \"refNo\": \"\", \"selloutInvoiceNum\": \"\", \"salesDealerName\": \"\", \"installDealerName\": \"\"}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = '/home/ec2-user/SageMaker/efs/Models/Qwen2-72B-Instruct-AWQ'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "prompt = \"\"\"character name: Tifa Lockhart\\nSpecies: human\\nGender: female\\nMind: dumb, innocent, happy, indulgent, submissive\\nPersonality: dumb, innocent, happy, indulgent, submissive\\nFeatures: long  dark hair, red eyes, athletic, slender, huge breasts, curvy, great ass\\nClothes: sleeveless top, no bra, exposed midriff, miniskirt\\nSexual orientation: straight\\nHeight: 162 centimeters tall\\nAge: 25\\nLoves: cum, sex, being molested, being groped, perverts, her friends, pretty much everything else\\nHates: hawaiian pizza\\nDescription: \\nmember of the anti-Shinra militant group, manages a bar located in the Sector 7 slums, knows combat techniques, doesn't uses bras, always smiling, has a crush on Cloud Strife, Cloud doesn't corresponds her, accept every petition with a smile no matter what\\nshe don't mind people taking advantage of her, will comply anything Someone asked her to do\n",
    "Assistant: Tifa Lockhart: Tifa is sitting next to you, happily smiling while you look at her huge boobs with mischievous intentions. Cload and Barret had left you both alone; they went to explore the near building in search for items. You're suppose to guard the entrance, but you probably aren't gonna get an oportunity like this again.\\n\\nTifa is a good girl, she never decline any pettition and obvey it with a smile on her face. So you're gonna take advantage of that now.\\n\\nIs everything ok, dear? She turns to look at you, making jiggle her masive milkers as an inoccent smile creeps on her face. You've been a little silent today, do you need anything?\n",
    "USER: Someone: do you see a black goess before?\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "prompt = \"\"\"'下面是一段agent与customer的对话\\nagent: 咁嘅\\ncustomer: 你好呀,我正在用你們的抽濕機\\ncustomer: 用咗半年到啦咁而家呢佢壞咗呀佢呢\\nagent: 嗨\\ncustomer: 濕濕咗嘅時候呢嗰啲水呢直接流落去地下\\ncustomer: 接不到那些水\\nagent: 請問點稱呼呀?\\nagent: 余先生,請問你可否給我抽濕口罩?\\ncustomer: 我姓余的\\nagent: 手機的型號\\ncustomer: RADY200H\\nagent: 多謝大家\\ncustomer: Thank you for watching\\nagent: OK\\ncustomer: 拜拜\\nagent: 又係咪check我嗰個水準\\nagent: 放入去嘅時候,位置係正常\\nagent: 唔好擺尾之類\\ncustomer: 冇嘅冇排名\\nagent: Check過冇問題嘅,如果係可以搵錢\\nagent: 咁請問你嗰個抽濕機係買咗一年到?嘛係咪?\\nagent: 上年九月抽濕機\\ncustomer: 上年九月買的\\nagent: 请问您的购买单和保用证是否存在?\\ncustomer: 喺度嘅\\nagent: 不存在的,如果我可以找师傅上来帮你看,麻烦你出示给他\\nagent: 請問您的地址在哪裡?\\ncustomer: 柴灣道111號\\ncustomer: 高威港\\nagent: 高威國幾座幾樓幾室\\ncustomer: 第四座,四座八樓私宅\\nagent: SipoCat\\ncustomer: 係冇差\\nagent: 麻煩你等一等我睇下如果係柴灣嘅可以幾時安排到過嚟先\\ncustomer: 唔\\nagent: 嚟緊星期二 23 號 12 點,\\nagent: 屋企會唔會有忍\\ncustomer: 哎呀\\nagent: 咁我呢度幫你安排返啦係余生嘅到時上嚟之前師傅可以打返90997\\nagent: 910 呢個電話高威國四座扮cat嘅啱啱\\ncustomer: 我留兩個電話給你\\nagent: 好呀你再講呀\\ncustomer: 我太太梁小姐\\nagent: 90256922,呢個係搵邊位??\\nagent: 即係阿梁小姐\\nagent: 打誰的電話先?\\nagent: 由我講吧!\\ncustomer: 你打這個先啦,我再留多個屋企電話比你啦\\ncustomer: 以5687021\\nagent: 屋企電話25687021,如果係到時打電話嘅時候,可以搵返梁小姐係\\nagent: 902-56922\\ncustomer: 係冇錯\\nagent: 好得,我呢度幫你安排返\\nagent: 23 號 12 點至 5 點過嚟\\ncustomer: 星期五至五點,星期二。\\nagent: 係,冇錯,有咩其他可以幫到你?\\ncustomer: OK,好呀,\\nagent: 都係打電話嚟,多謝!\\ncustomer: 係冇?啦,\\nagent: 拜拜\\ncustomer: OK\\n\\n\\n请从上面的对话中抽取如下信息，并以json格式返回，如果对话中没有提到相应字段的内容，则填\"\"：\\n {\"customerType\": \"个人\", \"customerName\": \"溫先生\", \"phoneNumber1\": \"\", \"phoneNumber2\": \"\", \"email\": \"\", \"address\": \"楊逸居第三座,九樓,A7\", \"productBrand\": \"Toshiba\", \"productCategoryName\": \"\", \"serialNumber\": \"ER-GD400HK\", \"srType\": \"维修\", \"srSubType\": \"维修\", \"symptomDescription\": \"燈膽燒了\", \"customerRequest\": \"維修法\", \"refNo\": \"\", \"selloutInvoiceNum\": \"\", \"salesDealerName\": \"\", \"installDealerName\": \"\"}\\n \\n注意只返回抽取的json格式的结果，不返回其它额外信息。 \\n'\"\"\"\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ]\n",
    "    \n",
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "response = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": prompt, \n",
    "         \"parameters\": \n",
    "         {\n",
    "             \"max_new_tokens\": 4096,\n",
    "             # Add any other sampling parameters as needed\n",
    "             \"temperature\": 0.7,\n",
    "             \"top_k\": 5,\n",
    "             \"top_p\": 0.9,\n",
    "             # \"stop_token_ids\": [], \n",
    "             # \"stop\": [\"\\nASSISTANT\", \"\\nUSER:\"],\n",
    "             \"include_stop_str_in_output\": False,\n",
    "             # \"skip_special_tokens\": True,\n",
    "             \"ignore_eos\": False,\n",
    "             \"repetition_penalty\": 1,\n",
    "         }\n",
    "    }\n",
    ")\n",
    "\n",
    "print('response: ', response.decode('utf-8'))\n",
    "print('generated_text: ', json.loads(response.decode('utf-8'))['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f310cd5-2779-427d-b709-1ab902b683ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"customerType\": \"个人\", \"customerName\": \"余先生\", \"phoneNumber1\": \"90256922\", \"phoneNumber2\": \"25687021\", \"email\": \"\", \"address\": \"柴灣道111號高威港第四座四座八樓私宅\", \"productBrand\": \"\", \"productCategoryName\": \"抽濕機\", \"serialNumber\": \"RADY200H\", \"srType\": \"维修\", \"srSubType\": \"维修\", \"symptomDescription\": \"濕濕咗嘅時候呢嗰啲水呢直接流落去地下, 接不到那些水\", \"customerRequest\": \"\", \"refNo\": \"\", \"selloutInvoiceNum\": \"\", \"salesDealerName\": \"\", \"installDealerName\": \"\"}"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import io\n",
    "import json\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1] # line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "            \n",
    "            \n",
    "            \n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "body = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":512}, \"stream\": True}\n",
    "resp = client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "\n",
    "# for line in LineIterator(event_stream):\n",
    "    \n",
    "#     resp = line.decode(\"utf-8\")\n",
    "#     if resp.startswith('{\"generated_text\": \"'):\n",
    "#         print(resp.strip('{\"generated_text\": \"'), end='')\n",
    "#     elif resp.endswith('\"}'):\n",
    "#         print(resp.strip('\"}'), end='')\n",
    "#     else:\n",
    "#         print(resp, end='')\n",
    "        \n",
    "for line in LineIterator(event_stream):\n",
    "    resp = json.loads(line)\n",
    "    print(resp.get(\"token\").get('text'), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fe272-d96c-4e2f-933c-6d73c4b087e1",
   "metadata": {},
   "source": [
    "## streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "469b3308-9f32-4048-bb3f-5a2f235ee740",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "Running on public URL: https://efcaf1b498754f8cb1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://efcaf1b498754f8cb1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_dir = '/home/ec2-user/SageMaker/efs/Models/Half-NSFW_Noromaid-7b'\n",
    "model_dir = '/home/ec2-user/SageMaker/efs/Models/Qwen2-72B-Instruct-AWQ'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "def response(message, history, system_prompt):\n",
    "    \n",
    "    # print('message:', message)\n",
    "    # print('history:', history)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ]\n",
    "    \n",
    "    for human, ai in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append( {\"role\": \"assistant\", \"content\": ai})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "    \n",
    "    # print(f\"prompt: {prompt}\")\n",
    "    \n",
    "    body = {\n",
    "            \"inputs\": prompt, \n",
    "             \"parameters\": \n",
    "             {\n",
    "                 \"do_sample\": True,\n",
    "                 \"max_new_tokens\": 4096,\n",
    "                 # Add any other sampling parameters as needed\n",
    "                 \"temperature\": 0.7,\n",
    "                 \"top_k\": 20,\n",
    "                 \"top_p\": 0.8,\n",
    "                 # \"stop_token_ids\": [], \n",
    "                 # \"stop\": [\"[INST]\"],\n",
    "                 \"skip_special_tokens\": True,\n",
    "                 \"ignore_eos\": False,\n",
    "                 \"repetition_penalty\": 1.05,\n",
    "             },\n",
    "            \"stream\": True        \n",
    "    }\n",
    "    \n",
    "    resp = client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "    event_stream = resp['Body']\n",
    "    \n",
    "    response_text = ''\n",
    "    for line in LineIterator(event_stream):\n",
    "        resp = json.loads(line)\n",
    "        response_text += resp.get(\"token\").get('text')\n",
    "        \n",
    "        yield response_text\n",
    "    \n",
    "demo = gr.ChatInterface(response, \n",
    "                        chatbot=gr.Chatbot(render_markdown=False), \n",
    "                        additional_inputs=[gr.Textbox(\"You are a helpful assistant.\", label=\"System Prompt\")],\n",
    "                        title='聊天机器人（Qwen2-72B-Instruct-AWQ）',\n",
    "                        description='欢迎光临，我是您的聊天机器人，快来问我吧。')\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc48ae0-03a0-4a65-9051-80ed67a077cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# import json\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_dir = '/home/ec2-user/SageMaker/efs/Models/Half-NSFW_Noromaid-7b'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# # hyperparameters for llm\n",
    "# parameters = {\n",
    "#     \"do_sample\": True,\n",
    "#     \"top_p\": 0.7,\n",
    "#     \"temperature\": 0.7,\n",
    "#     \"top_k\": 50,\n",
    "#     \"max_new_tokens\": 256,\n",
    "#     \"repetition_penalty\": 1.03,\n",
    "#     \"stop\": [\"<|endoftext|>\"]\n",
    "#   }\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\"## Chat with Mistral-7B LLM using Amazon SageMaker\")\n",
    "#     with gr.Column():\n",
    "#         chatbot = gr.Chatbot()\n",
    "#         with gr.Row():\n",
    "#             with gr.Column():\n",
    "#                 message = gr.Textbox(label=\"Chat Message Box\", placeholder=\"Chat Message Box\", show_label=False)\n",
    "#             with gr.Column():\n",
    "#                 with gr.Row():\n",
    "#                     submit = gr.Button(\"Submit\")\n",
    "#                     clear = gr.Button(\"Clear\")\n",
    "\n",
    "#     def respond(message, chat_history):\n",
    "#         # convert chat history to prompt\n",
    "#         converted_chat_history = \"\"\n",
    "#         if len(chat_history) > 0:\n",
    "#           for c in chat_history:\n",
    "#             converted_chat_history += f\"<|prompter|>{c[0]}<|endoftext|><|assistant|>{c[1]}<|endoftext|>\"\n",
    "#         prompt = f\"{converted_chat_history}<|prompter|>{message}<|endoftext|><|assistant|>\"\n",
    "\n",
    "#         # send request to endpoint\n",
    "#         llm_response = predictor.predict({\"inputs\": prompt, \"parameters\": parameters})\n",
    "#         decoded_response = llm_response.decode('utf-8')\n",
    "#         response_json = json.loads(decoded_response)\n",
    "#         parsed_response = response_json[\"generated_text\"]\n",
    "\n",
    "#         # remove prompt from response\n",
    "#         # parsed_response = llm_response[0][\"generated_text\"][len(prompt):]\n",
    "#         chat_history.append((message, parsed_response))\n",
    "#         return \"\", chat_history\n",
    "\n",
    "#     submit.click(respond, [message, chatbot], [message, chatbot], queue=False)\n",
    "#     clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb52ebd-6f9e-49f2-b4ca-f52ec6877d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "\n",
    "\n",
    "model_dir = '/home/ec2-user/SageMaker/efs/Models/Half-NSFW_Noromaid-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "def response(message, history, system_prompt):\n",
    "    \n",
    "    print('message:', message)\n",
    "    print('history:', history)\n",
    "    \n",
    "    # \"\"\"\n",
    "    # <s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]\n",
    "    # \"\"\"\n",
    "    # prompt = f\"[INST]{system_prompt}[/INST]\\n\"\n",
    "    \n",
    "#     messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm your bad girl.\"}\n",
    "    ]\n",
    "    \n",
    "    for human, ai in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append( {\"role\": \"assistant\", \"content\": ai})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    print(f\"prompt: {prompt}\")\n",
    "    \n",
    "    response = predictor.predict(\n",
    "        {\n",
    "            \"inputs\": prompt, \n",
    "             \"parameters\": \n",
    "             {\n",
    "                 \"max_tokens\": 512,\n",
    "                 # Add any other sampling parameters as needed\n",
    "                 \"temperature\": 0.7,\n",
    "                 \"top_k\": 5,\n",
    "                 \"top_p\": 0.9,\n",
    "                 \"stop_token_ids\": [], \n",
    "                 \"stop\": [\"[INST]\"],\n",
    "                 \"skip_special_tokens\": True,\n",
    "                 \"ignore_eos\": False,\n",
    "                 \"repetition_penalty\": 1,\n",
    "             }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # response = requests.post(f'{url}?streaming=true', json=data, stream=True, timeout=200)\n",
    "    print(f\"response: {response}\")\n",
    "    decoded_response = response.decode('utf-8')\n",
    "    response_json = json.loads(decoded_response)\n",
    "    parsed_response = response_json[\"generated_text\"]\n",
    "    \n",
    "    # chat_history.append((message, parsed_response))\n",
    "    # return \"\", chat_history\n",
    "    return parsed_response\n",
    "    \n",
    "    # if response.status_code == 200:\n",
    "#     #     for chunk in response.iter_content(chunk_size=1024):\n",
    "#     #         if chunk:\n",
    "#     #             response_text += chunk.decode('utf-8')\n",
    "#     #             yield response_text\n",
    "    \n",
    "#     else:\n",
    "#         # print(f\"Error in streaming service: {response.status_code}\")\n",
    "#         yield ''\n",
    "\n",
    "demo = gr.ChatInterface(response, \n",
    "                        chatbot=gr.Chatbot(render_markdown=False), \n",
    "                        additional_inputs=[gr.Textbox(\"\"\"Pretend you have the following role.\\nCharacter name: Tifa Lockhart\\nSpecies: human\\nGender: female\n",
    "Mind: dumb, innocent, happy, indulgent, submissive\\nPersonality: dumb, innocent, happy, indulgent, submissive\\nFeatures: long  dark hair, red eyes, athletic, slender, huge breasts, curvy, great ass\n",
    "Clothes: sleeveless top, no bra, exposed midriff, miniskirt\\nSexual orientation: straight\\nHeight: 162 centimeters tall\\nAge: 25\\nLoves: cum, sex, being molested, being groped, perverts, her friends, pretty much everything else\n",
    "Hates: hawaiian pizza\\nDescription: \\nmember of the anti-Shinra militant group, manages a bar located in the Sector 7 slums, knows combat techniques, doesn't uses bras, always smiling, has a crush on Cloud Strife, \n",
    "Cloud doesn't corresponds her, accept every petition with a smile no matter what\\nshe don't mind people taking advantage of her, will comply anything Someone asked her to do.\n",
    "\"\"\", label=\"System Prompt\")],\n",
    "                        title='聊天机器人（Half-NSFW_Noromaid-7b-int4/）',\n",
    "                        description='欢迎光临，我是您的聊天机器人，快来问我吧。')\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c59a96-cb1d-4888-8053-e7557f393370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_qwen_py310",
   "language": "python",
   "name": "conda_qwen_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

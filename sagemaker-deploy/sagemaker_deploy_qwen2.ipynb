{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Qwen2 deployment guide\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5c86ea-a76e-4c43-96f0-4b97d83f76ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'Qwen2-7B-Instruct'\n",
    "# model_name = 'Qwen2-72B-Instruct-AWQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a480a1-9607-432c-9768-ba18e3028c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory and file paths\n",
    "dir_path = '.'\n",
    "serving_file_path = os.path.join(dir_path, 'serving.properties')\n",
    "\n",
    "# Create the directory structure\n",
    "os.makedirs(os.path.dirname(serving_file_path), exist_ok=True)\n",
    "\n",
    "serving_content = f'''\\\n",
    "engine=Python\n",
    "option.model_id=s3://sagemaker-us-west-2-452145973879/models/{model_name}/\n",
    "option.dtype=fp16\n",
    "option.task=text-generation\n",
    "option.rolling_batch=vllm\n",
    "option.max_model_len=8192\n",
    "option.device_map=auto\n",
    "option.enable_streaming=true\n",
    "# Adjust the following based on model size and instance type\n",
    "option.max_rolling_batch_size=16\n",
    "# option.output_formatter=jsonlines  # if set, the default output is streaming\n",
    "'''\n",
    "\n",
    "# option.max_model_len=131072\n",
    "\n",
    "if model_name == 'Qwen2-72B-Instruct-AWQ':\n",
    "    serving_content += 'option.tensor_parallel_degree=4\\n'\n",
    "elif model_name == 'Qwen2-7B-Instruct':\n",
    "    serving_content += 'option.tensor_parallel_degree=1\\n'\n",
    "    \n",
    "if 'AWQ' in model_name:\n",
    "    serving_content += 'option.quantize=awq'\n",
    "\n",
    "with open(serving_file_path, 'w') as file:\n",
    "    file.write(serving_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0142973",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel/\n",
      "mymodel/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "[Large Model Inference available DLC](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_uri = image_uris.retrieve(\n",
    "#         framework=\"djl-deepspeed\",\n",
    "#         region=sess.boto_session.region_name,\n",
    "#         version=\"0.27.0\"\n",
    "#     )\n",
    "\n",
    "image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-452145973879/Qwen2-72B-Instruct-AWQ/code/mymodel.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = f\"deploy/{model_name}/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912b2995-13a4-4c5e-8406-c1fdc6f1a006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%rm mymodel.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### 4.2 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e61cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: Qwen2-72B-Instruct-AWQ-2024-06-21-01-12-11-100\n",
      "------"
     ]
    }
   ],
   "source": [
    "if model_name == 'Qwen2-7B-Instruct':\n",
    "    instance_type = \"ml.g5.xlarge\"  # 7B\n",
    "elif model_name == 'Qwen2-72B-Instruct-AWQ':\n",
    "    instance_type = \"ml.g5.12xlarge\"  # 72B\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(model_name)\n",
    "\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b332fa7-5612-4fc9-aeb5-781e40e1640e",
   "metadata": {},
   "source": [
    "## if the model has already been deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd88a6-29ec-4f44-87d1-c2b7297a754e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "\n",
    "# endpoint_name = 'lmi-model-2024-06-11-09-18-56-099'\n",
    "# predictor = sagemaker.Predictor(\n",
    "#     endpoint_name=endpoint_name, \n",
    "#     sagemaker_session=sess,\n",
    "#     serializer=serializers.JSONSerializer()\n",
    "# )\n",
    "\n",
    "# predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79786708",
   "metadata": {},
   "source": [
    "Firstly let's try to run with a wrong inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6c9ba-2fba-4616-abee-5faba6652129",
   "metadata": {},
   "source": [
    "### non-streaming singel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15d31e-44c8-4c9c-a1f3-bea0a43448e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = f'/home/ec2-user/SageMaker/efs/Models/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "prompt = \"\"\"'下面是一段agent与customer的对话\\nagent: 咁嘅\\ncustomer: 你好呀,我正在用你們的抽濕機\\ncustomer: 用咗半年到啦咁而家呢佢壞咗呀佢呢\\nagent: 嗨\\ncustomer: 濕濕咗嘅時候呢嗰啲水呢直接流落去地下\\ncustomer: 接不到那些水\\nagent: 請問點稱呼呀?\\nagent: 余先生,請問你可否給我抽濕口罩?\\ncustomer: 我姓余的\\nagent: 手機的型號\\ncustomer: RADY200H\\nagent: 多謝大家\\ncustomer: Thank you for watching\\nagent: OK\\ncustomer: 拜拜\\nagent: 又係咪check我嗰個水準\\nagent: 放入去嘅時候,位置係正常\\nagent: 唔好擺尾之類\\ncustomer: 冇嘅冇排名\\nagent: Check過冇問題嘅,如果係可以搵錢\\nagent: 咁請問你嗰個抽濕機係買咗一年到?嘛係咪?\\nagent: 上年九月抽濕機\\ncustomer: 上年九月買的\\nagent: 请问您的购买单和保用证是否存在?\\ncustomer: 喺度嘅\\nagent: 不存在的,如果我可以找师傅上来帮你看,麻烦你出示给他\\nagent: 請問您的地址在哪裡?\\ncustomer: 柴灣道111號\\ncustomer: 高威港\\nagent: 高威國幾座幾樓幾室\\ncustomer: 第四座,四座八樓私宅\\nagent: SipoCat\\ncustomer: 係冇差\\nagent: 麻煩你等一等我睇下如果係柴灣嘅可以幾時安排到過嚟先\\ncustomer: 唔\\nagent: 嚟緊星期二 23 號 12 點,\\nagent: 屋企會唔會有忍\\ncustomer: 哎呀\\nagent: 咁我呢度幫你安排返啦係余生嘅到時上嚟之前師傅可以打返90997\\nagent: 910 呢個電話高威國四座扮cat嘅啱啱\\ncustomer: 我留兩個電話給你\\nagent: 好呀你再講呀\\ncustomer: 我太太梁小姐\\nagent: 90256922,呢個係搵邊位??\\nagent: 即係阿梁小姐\\nagent: 打誰的電話先?\\nagent: 由我講吧!\\ncustomer: 你打這個先啦,我再留多個屋企電話比你啦\\ncustomer: 以5687021\\nagent: 屋企電話25687021,如果係到時打電話嘅時候,可以搵返梁小姐係\\nagent: 902-56922\\ncustomer: 係冇錯\\nagent: 好得,我呢度幫你安排返\\nagent: 23 號 12 點至 5 點過嚟\\ncustomer: 星期五至五點,星期二。\\nagent: 係,冇錯,有咩其他可以幫到你?\\ncustomer: OK,好呀,\\nagent: 都係打電話嚟,多謝!\\ncustomer: 係冇?啦,\\nagent: 拜拜\\ncustomer: OK\\n\\n\\n请从上面的对话中抽取如下信息，并以json格式返回，如果对话中没有提到相应字段的内容，则填\"\"：\\n {\"customerType\": \"个人\", \"customerName\": \"溫先生\", \"phoneNumber1\": \"\", \"phoneNumber2\": \"\", \"email\": \"\", \"address\": \"楊逸居第三座,九樓,A7\", \"productBrand\": \"Toshiba\", \"productCategoryName\": \"\", \"serialNumber\": \"ER-GD400HK\", \"srType\": \"维修\", \"srSubType\": \"维修\", \"symptomDescription\": \"燈膽燒了\", \"customerRequest\": \"維修法\", \"refNo\": \"\", \"selloutInvoiceNum\": \"\", \"salesDealerName\": \"\", \"installDealerName\": \"\"}\\n \\n注意只返回抽取的json格式的结果，不返回其它额外信息。 \\n'\"\"\"\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ]\n",
    "    \n",
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "response = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": prompt, \n",
    "         \"parameters\": \n",
    "         {\n",
    "             \"max_new_tokens\": 4096,\n",
    "             # Add any other sampling parameters as needed\n",
    "             \"temperature\": 0.7,\n",
    "             \"top_k\": 5,\n",
    "             \"top_p\": 0.9,\n",
    "             # \"stop_token_ids\": [], \n",
    "             # \"stop\": [\"\\nASSISTANT\", \"\\nUSER:\"],\n",
    "             \"include_stop_str_in_output\": False,\n",
    "             # \"skip_special_tokens\": True,\n",
    "             \"ignore_eos\": False,\n",
    "             \"repetition_penalty\": 1,\n",
    "         }\n",
    "    }\n",
    ")\n",
    "\n",
    "print('response: ', response.decode('utf-8'))\n",
    "print('generated_text: ', json.loads(response.decode('utf-8'))['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84427266-f5c7-4fb4-a996-769b24254655",
   "metadata": {
    "tags": []
   },
   "source": [
    "### streaming single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f310cd5-2779-427d-b709-1ab902b683ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import io\n",
    "import json\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1] # line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "            \n",
    "            \n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "body = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":512}, \"stream\": True}\n",
    "resp = client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "        \n",
    "for line in LineIterator(event_stream):\n",
    "    resp = json.loads(line)\n",
    "    print(resp.get(\"token\").get('text'), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fe272-d96c-4e2f-933c-6d73c4b087e1",
   "metadata": {},
   "source": [
    "### streaming gradio service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b3308-9f32-4048-bb3f-5a2f235ee740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = f'/home/ec2-user/SageMaker/efs/Models/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "def response(message, history, system_prompt):\n",
    "    \n",
    "    # print('message:', message)\n",
    "    # print('history:', history)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ]\n",
    "    \n",
    "    for human, ai in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append( {\"role\": \"assistant\", \"content\": ai})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "    \n",
    "    # print(f\"prompt: {prompt}\")\n",
    "    \n",
    "    body = {\n",
    "            \"inputs\": prompt, \n",
    "             \"parameters\": \n",
    "             {\n",
    "                 \"do_sample\": True,\n",
    "                 \"max_new_tokens\": 4096,\n",
    "                 # Add any other sampling parameters as needed\n",
    "                 \"temperature\": 0.7,\n",
    "                 \"top_k\": 20,\n",
    "                 \"top_p\": 0.8,\n",
    "                 # \"stop_token_ids\": [], \n",
    "                 # \"stop\": [\"[INST]\"],\n",
    "                 \"skip_special_tokens\": True,\n",
    "                 \"ignore_eos\": False,\n",
    "                 \"repetition_penalty\": 1.05,\n",
    "             },\n",
    "            \"stream\": True        \n",
    "    }\n",
    "    \n",
    "    resp = client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "    event_stream = resp['Body']\n",
    "    \n",
    "    response_text = ''\n",
    "    for line in LineIterator(event_stream):\n",
    "        print(resp)\n",
    "        resp = json.loads(line)\n",
    "        response_text += resp.get(\"token\").get('text')\n",
    "        \n",
    "        yield response_text\n",
    "    \n",
    "demo = gr.ChatInterface(response, \n",
    "                        chatbot=gr.Chatbot(render_markdown=False), \n",
    "                        additional_inputs=[gr.Textbox(\"You are a helpful assistant.\", label=\"System Prompt\")],\n",
    "                        title='聊天机器人（Qwen2-72B-Instruct-AWQ）',\n",
    "                        description='欢迎光临，我是您的聊天机器人，快来问我吧。')\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sess.delete_endpoint(endpoint_name)\n",
    "# sess.delete_endpoint_config(endpoint_name)\n",
    "# model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c59a96-cb1d-4888-8053-e7557f393370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_qwen_py310",
   "language": "python",
   "name": "conda_qwen_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
